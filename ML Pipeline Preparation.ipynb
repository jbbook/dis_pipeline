{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline Preparation\n",
    "Follow the instructions below to help you create your ML pipeline.\n",
    "### 1. Import libraries and load data from database.\n",
    "- Import Python libraries\n",
    "- Load dataset from database with [`read_sql_table`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_table.html)\n",
    "- Define feature and target variables X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jens/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jens/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/jens/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to /home/jens/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "nltk.download(['punkt', 'wordnet', 'averaged_perceptron_tagger', 'stopwords'])\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.multioutput import MultiOutputClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from database\n",
    "engine = create_engine('sqlite:///disaster_response_database.db')\n",
    "df = pd.read_sql('messages', engine)\n",
    "df.head()\n",
    "X = df['message']\n",
    "Y = df.drop(labels=['genre', 'id', 'message', 'original'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Write a tokenization function to process your text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['weather', 'update', 'cold', 'front', 'cuba', 'could', 'pas', 'haiti']\n",
      "Weather update - a cold front from Cuba that could pass over Haiti\n",
      "['hurricane']\n",
      "Is the Hurricane over or is it not over\n",
      "['looking', 'someone', 'name']\n",
      "Looking for someone but no name\n",
      "['un', 'report', 'leogane', '80', '90', 'destroyed', 'hospital', 'st', 'croix', 'functioning', 'needs', 'supply', 'desperately']\n",
      "UN reports Leogane 80-90 destroyed. Only Hospital St. Croix functioning. Needs supplies desperately.\n",
      "['say', 'west', 'side', 'haiti', 'rest', 'country', 'today', 'tonight']\n",
      "says: west side of Haiti, rest of the country today and tonight\n",
      "['information', 'national', 'palace']\n",
      "Information about the National Palace-\n",
      "['storm', 'sacred', 'heart', 'jesus']\n",
      "Storm at sacred heart of jesus\n",
      "['please', 'need', 'tent', 'water', 'silo', 'thank']\n",
      "Please, we need tents and water. We are in Silo, Thank you!\n",
      "['would', 'like', 'receive', 'message', 'thank']\n",
      "I would like to receive the messages, thank you\n",
      "['croix', 'de', 'bouquets', 'health', 'issue', 'worker', 'santo', '15', 'area', 'croix', 'de', 'bouquets']\n",
      "I am in Croix-des-Bouquets. We have health issues. They ( workers ) are in Santo 15. ( an area in Croix-des-Bouquets )\n"
     ]
    }
   ],
   "source": [
    "url_regex = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "\n",
    "def tokenize(text):\n",
    "    detected_urls = re.findall(url_regex, text)\n",
    "    for url in detected_urls:\n",
    "        text = text.replace(url, 'urlplaceholder')\n",
    "    text = re.sub(r\"\\\\\", \"\", text)    \n",
    "    text = re.sub(r\"\\'\", \"\", text)    \n",
    "    text = re.sub(r\"\\\"\", \"\", text)\n",
    "    #replace punctuation characters with spaces\n",
    "    filters='!\"\\'#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\n",
    "    translate_dict = dict((c, \" \") for c in filters)\n",
    "    translate_map = str.maketrans(translate_dict)\n",
    "    text = text.translate(translate_map)\n",
    "    \n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    clean_tokens = []\n",
    " \n",
    "    \n",
    "    for tok in tokens:\n",
    "        clean_tok = lemmatizer.lemmatize(tok).lower().strip()\n",
    "        clean_tokens.append(clean_tok)\n",
    "        \n",
    "    filtered_tokens = [w for w in clean_tokens if not w in stop_words] \n",
    "        \n",
    "    return filtered_tokens\n",
    "\n",
    "for i in range(10):\n",
    "    print(tokenize(df['message'][i]))\n",
    "    print(df['message'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['thats', 'test']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(\"that's a test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build a machine learning pipeline\n",
    "This machine pipeline should take in the `message` column as input and output classification results on the other 36 categories in the dataset. You may find the [MultiOutputClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputClassifier.html) helpful for predicting multiple target variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer(tokenizer=tokenize)), \n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultiOutputClassifier(RandomForestClassifier()))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('vect',\n",
      "                 CountVectorizer(tokenizer=<function tokenize at 0x7f8d83c08620>)),\n",
      "                ('tfidf', TfidfTransformer()),\n",
      "                ('clf',\n",
      "                 MultiOutputClassifier(estimator=RandomForestClassifier()))])\n"
     ]
    }
   ],
   "source": [
    "print(pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train pipeline\n",
    "- Split data into train and test sets\n",
    "- Train pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'clf__estimator__n_estimators': 10,\n",
    "    'clf__estimator__criterion': 'gini',\n",
    "    'clf__estimator__max_features': ('auto'),#, 'log2'),\n",
    "    'clf__estimator__min_samples_split': 2,\n",
    "    'vect__ngram_range':  (1, 1),\n",
    "    'tfidf__use_idf': False,\n",
    "    'vect__max_df': 1.\n",
    "}\n",
    "\n",
    "pipeline.set_params(**parameters)\n",
    "\n",
    "pipeline.fit(X_train, Y_train)\n",
    "Y_pred = pipeline.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Y_pred = cv.best_estimator_.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Test your model\n",
    "Report the f1 score, precision and recall for each output category of the dataset. You can do this by iterating through the columns and calling sklearn's `classification_report` on each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2522918258212376\n"
     ]
    }
   ],
   "source": [
    "#print(cv.best_estimator_.score(X_test, Y_test))\n",
    "print(pipeline.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test_array = Y_test.to_numpy().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "related\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.67      0.45      0.54      1246\n",
      "        True       0.84      0.93      0.88      3990\n",
      "\n",
      "    accuracy                           0.82      5236\n",
      "   macro avg       0.76      0.69      0.71      5236\n",
      "weighted avg       0.80      0.82      0.80      5236\n",
      "\n",
      "roc auc score 0.6892300977162007\n",
      "[[ 559  687]\n",
      " [ 280 3710]]\n",
      "request\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.88      0.98      0.93      4291\n",
      "        True       0.79      0.42      0.55       945\n",
      "\n",
      "    accuracy                           0.88      5236\n",
      "   macro avg       0.84      0.70      0.74      5236\n",
      "weighted avg       0.87      0.88      0.86      5236\n",
      "\n",
      "roc auc score 0.6994053013628869\n",
      "[[4186  105]\n",
      " [ 545  400]]\n",
      "offer\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       1.00      1.00      1.00      5211\n",
      "        True       0.00      0.00      0.00        25\n",
      "\n",
      "    accuracy                           1.00      5236\n",
      "   macro avg       0.50      0.50      0.50      5236\n",
      "weighted avg       0.99      1.00      0.99      5236\n",
      "\n",
      "roc auc score 0.5\n",
      "[[5211    0]\n",
      " [  25    0]]\n",
      "aid_related\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.75      0.85      0.80      3028\n",
      "        True       0.75      0.61      0.67      2208\n",
      "\n",
      "    accuracy                           0.75      5236\n",
      "   macro avg       0.75      0.73      0.73      5236\n",
      "weighted avg       0.75      0.75      0.74      5236\n",
      "\n",
      "roc auc score 0.730036566921295\n",
      "[[2567  461]\n",
      " [ 856 1352]]\n",
      "medical_help\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.92      0.99      0.95      4788\n",
      "        True       0.49      0.08      0.14       448\n",
      "\n",
      "    accuracy                           0.91      5236\n",
      "   macro avg       0.70      0.54      0.55      5236\n",
      "weighted avg       0.88      0.91      0.89      5236\n",
      "\n",
      "roc auc score 0.5372219611528822\n",
      "[[4749   39]\n",
      " [ 411   37]]\n",
      "medical_products\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.95      1.00      0.98      4960\n",
      "        True       0.79      0.12      0.21       276\n",
      "\n",
      "    accuracy                           0.95      5236\n",
      "   macro avg       0.87      0.56      0.59      5236\n",
      "weighted avg       0.94      0.95      0.94      5236\n",
      "\n",
      "roc auc score 0.5606869448340346\n",
      "[[4951    9]\n",
      " [ 242   34]]\n",
      "search_and_rescue\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.97      1.00      0.99      5086\n",
      "        True       0.50      0.05      0.10       150\n",
      "\n",
      "    accuracy                           0.97      5236\n",
      "   macro avg       0.74      0.53      0.54      5236\n",
      "weighted avg       0.96      0.97      0.96      5236\n",
      "\n",
      "roc auc score 0.5258801939965919\n",
      "[[5078    8]\n",
      " [ 142    8]]\n",
      "security\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.98      1.00      0.99      5148\n",
      "        True       0.00      0.00      0.00        88\n",
      "\n",
      "    accuracy                           0.98      5236\n",
      "   macro avg       0.49      0.50      0.50      5236\n",
      "weighted avg       0.97      0.98      0.97      5236\n",
      "\n",
      "roc auc score 0.4999028749028749\n",
      "[[5147    1]\n",
      " [  88    0]]\n",
      "military\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.97      1.00      0.98      5067\n",
      "        True       0.68      0.09      0.16       169\n",
      "\n",
      "    accuracy                           0.97      5236\n",
      "   macro avg       0.83      0.54      0.57      5236\n",
      "weighted avg       0.96      0.97      0.96      5236\n",
      "\n",
      "roc auc score 0.543687954194854\n",
      "[[5060    7]\n",
      " [ 154   15]]\n",
      "water\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.95      1.00      0.97      4878\n",
      "        True       0.85      0.26      0.40       358\n",
      "\n",
      "    accuracy                           0.95      5236\n",
      "   macro avg       0.90      0.63      0.69      5236\n",
      "weighted avg       0.94      0.95      0.93      5236\n",
      "\n",
      "roc auc score 0.6282482517562606\n",
      "[[4862   16]\n",
      " [ 265   93]]\n",
      "food\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.94      0.99      0.96      4655\n",
      "        True       0.82      0.49      0.61       581\n",
      "\n",
      "    accuracy                           0.93      5236\n",
      "   macro avg       0.88      0.74      0.79      5236\n",
      "weighted avg       0.93      0.93      0.92      5236\n",
      "\n",
      "roc auc score 0.737746690305799\n",
      "[[4593   62]\n",
      " [ 297  284]]\n",
      "shelter\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.93      0.99      0.96      4756\n",
      "        True       0.79      0.29      0.42       480\n",
      "\n",
      "    accuracy                           0.93      5236\n",
      "   macro avg       0.86      0.64      0.69      5236\n",
      "weighted avg       0.92      0.93      0.91      5236\n",
      "\n",
      "roc auc score 0.6409018432856742\n",
      "[[4719   37]\n",
      " [ 341  139]]\n",
      "clothing\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.99      1.00      0.99      5162\n",
      "        True       0.56      0.07      0.12        74\n",
      "\n",
      "    accuracy                           0.99      5236\n",
      "   macro avg       0.77      0.53      0.56      5236\n",
      "weighted avg       0.98      0.99      0.98      5236\n",
      "\n",
      "roc auc score 0.5333963370577086\n",
      "[[5158    4]\n",
      " [  69    5]]\n",
      "money\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.98      1.00      0.99      5127\n",
      "        True       0.86      0.06      0.10       109\n",
      "\n",
      "    accuracy                           0.98      5236\n",
      "   macro avg       0.92      0.53      0.55      5236\n",
      "weighted avg       0.98      0.98      0.97      5236\n",
      "\n",
      "roc auc score 0.5274254128619309\n",
      "[[5126    1]\n",
      " [ 103    6]]\n",
      "missing_people\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.99      1.00      0.99      5180\n",
      "        True       1.00      0.05      0.10        56\n",
      "\n",
      "    accuracy                           0.99      5236\n",
      "   macro avg       0.99      0.53      0.55      5236\n",
      "weighted avg       0.99      0.99      0.99      5236\n",
      "\n",
      "roc auc score 0.5267857142857143\n",
      "[[5180    0]\n",
      " [  53    3]]\n",
      "refugees\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.96      1.00      0.98      5038\n",
      "        True       0.56      0.03      0.05       198\n",
      "\n",
      "    accuracy                           0.96      5236\n",
      "   macro avg       0.76      0.51      0.51      5236\n",
      "weighted avg       0.95      0.96      0.95      5236\n",
      "\n",
      "roc auc score 0.5122292796965286\n",
      "[[5034    4]\n",
      " [ 193    5]]\n",
      "death\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.96      1.00      0.98      4998\n",
      "        True       0.84      0.15      0.26       238\n",
      "\n",
      "    accuracy                           0.96      5236\n",
      "   macro avg       0.90      0.57      0.62      5236\n",
      "weighted avg       0.96      0.96      0.95      5236\n",
      "\n",
      "roc auc score 0.5749299719887955\n",
      "[[4991    7]\n",
      " [ 202   36]]\n",
      "other_aid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jens/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/jens/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/jens/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.87      0.99      0.93      4539\n",
      "        True       0.46      0.04      0.08       697\n",
      "\n",
      "    accuracy                           0.87      5236\n",
      "   macro avg       0.67      0.52      0.50      5236\n",
      "weighted avg       0.82      0.87      0.81      5236\n",
      "\n",
      "roc auc score 0.5176653286691492\n",
      "[[4504   35]\n",
      " [ 667   30]]\n",
      "infrastructure_related\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.93      1.00      0.96      4881\n",
      "        True       0.22      0.01      0.01       355\n",
      "\n",
      "    accuracy                           0.93      5236\n",
      "   macro avg       0.58      0.50      0.49      5236\n",
      "weighted avg       0.88      0.93      0.90      5236\n",
      "\n",
      "roc auc score 0.5020998352334866\n",
      "[[4874    7]\n",
      " [ 353    2]]\n",
      "transport\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.96      1.00      0.98      4994\n",
      "        True       0.58      0.07      0.13       242\n",
      "\n",
      "    accuracy                           0.95      5236\n",
      "   macro avg       0.77      0.54      0.55      5236\n",
      "weighted avg       0.94      0.95      0.94      5236\n",
      "\n",
      "roc auc score 0.535888520770379\n",
      "[[4981   13]\n",
      " [ 224   18]]\n",
      "buildings\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.95      1.00      0.97      4947\n",
      "        True       0.75      0.15      0.25       289\n",
      "\n",
      "    accuracy                           0.95      5236\n",
      "   macro avg       0.85      0.57      0.61      5236\n",
      "weighted avg       0.94      0.95      0.93      5236\n",
      "\n",
      "roc auc score 0.5729794646785337\n",
      "[[4933   14]\n",
      " [ 246   43]]\n",
      "electricity\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.98      1.00      0.99      5140\n",
      "        True       0.50      0.02      0.04        96\n",
      "\n",
      "    accuracy                           0.98      5236\n",
      "   macro avg       0.74      0.51      0.52      5236\n",
      "weighted avg       0.97      0.98      0.97      5236\n",
      "\n",
      "roc auc score 0.5102221141374839\n",
      "[[5138    2]\n",
      " [  94    2]]\n",
      "tools\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.99      1.00      1.00      5207\n",
      "        True       0.00      0.00      0.00        29\n",
      "\n",
      "    accuracy                           0.99      5236\n",
      "   macro avg       0.50      0.50      0.50      5236\n",
      "weighted avg       0.99      0.99      0.99      5236\n",
      "\n",
      "roc auc score 0.5\n",
      "[[5207    0]\n",
      " [  29    0]]\n",
      "hospitals\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.99      1.00      0.99      5177\n",
      "        True       0.00      0.00      0.00        59\n",
      "\n",
      "    accuracy                           0.99      5236\n",
      "   macro avg       0.49      0.50      0.50      5236\n",
      "weighted avg       0.98      0.99      0.98      5236\n",
      "\n",
      "roc auc score 0.5\n",
      "[[5177    0]\n",
      " [  59    0]]\n",
      "shops\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.99      1.00      1.00      5209\n",
      "        True       0.00      0.00      0.00        27\n",
      "\n",
      "    accuracy                           0.99      5236\n",
      "   macro avg       0.50      0.50      0.50      5236\n",
      "weighted avg       0.99      0.99      0.99      5236\n",
      "\n",
      "roc auc score 0.5\n",
      "[[5209    0]\n",
      " [  27    0]]\n",
      "aid_centers\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.99      1.00      0.99      5173\n",
      "        True       0.00      0.00      0.00        63\n",
      "\n",
      "    accuracy                           0.99      5236\n",
      "   macro avg       0.49      0.50      0.50      5236\n",
      "weighted avg       0.98      0.99      0.98      5236\n",
      "\n",
      "roc auc score 0.5\n",
      "[[5173    0]\n",
      " [  63    0]]\n",
      "other_infrastructure\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.95      1.00      0.98      4999\n",
      "        True       0.00      0.00      0.00       237\n",
      "\n",
      "    accuracy                           0.95      5236\n",
      "   macro avg       0.48      0.50      0.49      5236\n",
      "weighted avg       0.91      0.95      0.93      5236\n",
      "\n",
      "roc auc score 0.499499899979996\n",
      "[[4994    5]\n",
      " [ 237    0]]\n",
      "weather_related\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.86      0.96      0.91      3804\n",
      "        True       0.85      0.59      0.70      1432\n",
      "\n",
      "    accuracy                           0.86      5236\n",
      "   macro avg       0.86      0.77      0.80      5236\n",
      "weighted avg       0.86      0.86      0.85      5236\n",
      "\n",
      "roc auc score 0.7745000851793761\n",
      "[[3661  143]\n",
      " [ 592  840]]\n",
      "floods\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.94      1.00      0.97      4822\n",
      "        True       0.90      0.31      0.46       414\n",
      "\n",
      "    accuracy                           0.94      5236\n",
      "   macro avg       0.92      0.65      0.72      5236\n",
      "weighted avg       0.94      0.94      0.93      5236\n",
      "\n",
      "roc auc score 0.654241730234012\n",
      "[[4807   15]\n",
      " [ 285  129]]\n",
      "storm\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.94      0.99      0.96      4733\n",
      "        True       0.79      0.41      0.54       503\n",
      "\n",
      "    accuracy                           0.93      5236\n",
      "   macro avg       0.87      0.70      0.75      5236\n",
      "weighted avg       0.93      0.93      0.92      5236\n",
      "\n",
      "roc auc score 0.6999551392259165\n",
      "[[4678   55]\n",
      " [ 296  207]]\n",
      "fire\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.99      1.00      1.00      5185\n",
      "        True       1.00      0.02      0.04        51\n",
      "\n",
      "    accuracy                           0.99      5236\n",
      "   macro avg       1.00      0.51      0.52      5236\n",
      "weighted avg       0.99      0.99      0.99      5236\n",
      "\n",
      "roc auc score 0.5098039215686274\n",
      "[[5185    0]\n",
      " [  50    1]]\n",
      "earthquake\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.97      0.99      0.98      4784\n",
      "        True       0.85      0.70      0.77       452\n",
      "\n",
      "    accuracy                           0.96      5236\n",
      "   macro avg       0.91      0.85      0.88      5236\n",
      "weighted avg       0.96      0.96      0.96      5236\n",
      "\n",
      "roc auc score 0.8461260987953947\n",
      "[[4730   54]\n",
      " [ 134  318]]\n",
      "cold\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.98      1.00      0.99      5130\n",
      "        True       0.83      0.05      0.09       106\n",
      "\n",
      "    accuracy                           0.98      5236\n",
      "   macro avg       0.91      0.52      0.54      5236\n",
      "weighted avg       0.98      0.98      0.97      5236\n",
      "\n",
      "roc auc score 0.5234874397734378\n",
      "[[5129    1]\n",
      " [ 101    5]]\n",
      "other_weather\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.95      1.00      0.97      4962\n",
      "        True       0.59      0.04      0.07       274\n",
      "\n",
      "    accuracy                           0.95      5236\n",
      "   macro avg       0.77      0.52      0.52      5236\n",
      "weighted avg       0.93      0.95      0.93      5236\n",
      "\n",
      "roc auc score 0.5175428144408454\n",
      "[[4955    7]\n",
      " [ 264   10]]\n",
      "direct_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.86      0.98      0.91      4203\n",
      "        True       0.76      0.33      0.46      1033\n",
      "\n",
      "    accuracy                           0.85      5236\n",
      "   macro avg       0.81      0.65      0.69      5236\n",
      "weighted avg       0.84      0.85      0.82      5236\n",
      "\n",
      "roc auc score 0.6515941109690009\n",
      "[[4098  105]\n",
      " [ 694  339]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jens/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/jens/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/jens/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/jens/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/jens/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/jens/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/jens/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/jens/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/jens/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/jens/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/jens/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/jens/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "roc_score = []\n",
    "for column in range(35):\n",
    "    print(Y.columns[column])\n",
    "    print(classification_report(Y_test_array[column], Y_pred.T[column]))\n",
    "    roc = roc_auc_score(Y_test_array[column], Y_pred.T[column])\n",
    "    print('roc auc score', roc)\n",
    "    roc_score.append(roc)\n",
    "    print(confusion_matrix(Y_test_array[column], Y_pred.T[column]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average roc 0.5795234828564477\n"
     ]
    }
   ],
   "source": [
    "print('average roc', np.mean(roc_score)) #0.83"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Improve your model\n",
    "Use grid search to find better parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jens/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:921: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "  category=UserWarning\n"
     ]
    }
   ],
   "source": [
    "\n",
    "parameters = {\n",
    "    'clf__estimator__n_estimators': [10],\n",
    "    'clf__estimator__criterion': ['gini', 'entropy'],\n",
    "    'clf__estimator__max_features': ['auto', 'sqrt'],#, 'log2'),\n",
    "    'clf__estimator__min_samples_split': [2],#, 3),#,\n",
    "    'vect__ngram_range': ((1, 1),(1,2)),#, (1, 2), (2,2)),\n",
    "    'tfidf__use_idf': [True, False],\n",
    "    'vect__max_df': [1.],\n",
    "    #'vect__max_df': (0.1, 0.5, 1.0)\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, param_grid=parameters, n_jobs=6, scoring='roc_auc_ovo', verbose=1)\n",
    "grid_fit = grid_search.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Test your model\n",
    "Show the accuracy, precision, and recall of the tuned model.  \n",
    "\n",
    "Since this project focuses on code quality, process, and  pipelines, there is no minimum performance metric needed to pass. However, make sure to fine tune your models for accuracy, precision and recall to make your project stand out - especially for your portfolio!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf__estimator__criterion': 'gini',\n",
       " 'clf__estimator__max_features': 'auto',\n",
       " 'clf__estimator__min_samples_split': 2,\n",
       " 'clf__estimator__n_estimators': 10,\n",
       " 'tfidf__use_idf': True,\n",
       " 'vect__max_df': 1.0,\n",
       " 'vect__ngram_range': (1, 1)}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_fit.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Try improving your model further. Here are a few ideas:\n",
    "* try other machine learning algorithms\n",
    "* add other features besides the TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = grid_fit.best_estimator_.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "related\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.64      0.47      0.54      1246\n",
      "        True       0.85      0.92      0.88      3990\n",
      "\n",
      "    accuracy                           0.81      5236\n",
      "   macro avg       0.74      0.69      0.71      5236\n",
      "weighted avg       0.80      0.81      0.80      5236\n",
      "\n",
      "roc auc score 0.6926952211990651\n",
      "[[ 582  664]\n",
      " [ 326 3664]]\n",
      "request\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.88      0.98      0.93      4291\n",
      "        True       0.81      0.39      0.53       945\n",
      "\n",
      "    accuracy                           0.87      5236\n",
      "   macro avg       0.84      0.69      0.73      5236\n",
      "weighted avg       0.87      0.87      0.86      5236\n",
      "\n",
      "roc auc score 0.6863383308734042\n",
      "[[4201   90]\n",
      " [ 573  372]]\n",
      "offer\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       1.00      1.00      1.00      5211\n",
      "        True       0.00      0.00      0.00        25\n",
      "\n",
      "    accuracy                           1.00      5236\n",
      "   macro avg       0.50      0.50      0.50      5236\n",
      "weighted avg       0.99      1.00      0.99      5236\n",
      "\n",
      "roc auc score 0.5\n",
      "[[5211    0]\n",
      " [  25    0]]\n",
      "aid_related\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.74      0.86      0.80      3028\n",
      "        True       0.76      0.59      0.67      2208\n",
      "\n",
      "    accuracy                           0.75      5236\n",
      "   macro avg       0.75      0.73      0.73      5236\n",
      "weighted avg       0.75      0.75      0.74      5236\n",
      "\n",
      "roc auc score 0.7272533647311087\n",
      "[[2605  423]\n",
      " [ 896 1312]]\n",
      "medical_help\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.92      0.99      0.96      4788\n",
      "        True       0.51      0.06      0.11       448\n",
      "\n",
      "    accuracy                           0.91      5236\n",
      "   macro avg       0.71      0.53      0.53      5236\n",
      "weighted avg       0.88      0.91      0.88      5236\n",
      "\n",
      "roc auc score 0.5274188074352548\n",
      "[[4762   26]\n",
      " [ 421   27]]\n",
      "medical_products\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.95      1.00      0.97      4960\n",
      "        True       0.70      0.09      0.17       276\n",
      "\n",
      "    accuracy                           0.95      5236\n",
      "   macro avg       0.83      0.55      0.57      5236\n",
      "weighted avg       0.94      0.95      0.93      5236\n",
      "\n",
      "roc auc score 0.5459925783076204\n",
      "[[4949   11]\n",
      " [ 250   26]]\n",
      "search_and_rescue\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.97      1.00      0.99      5086\n",
      "        True       0.69      0.12      0.20       150\n",
      "\n",
      "    accuracy                           0.97      5236\n",
      "   macro avg       0.83      0.56      0.60      5236\n",
      "weighted avg       0.97      0.97      0.96      5236\n",
      "\n",
      "roc auc score 0.5592135273299254\n",
      "[[5078    8]\n",
      " [ 132   18]]\n",
      "security\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.98      1.00      0.99      5148\n",
      "        True       0.00      0.00      0.00        88\n",
      "\n",
      "    accuracy                           0.98      5236\n",
      "   macro avg       0.49      0.50      0.50      5236\n",
      "weighted avg       0.97      0.98      0.97      5236\n",
      "\n",
      "roc auc score 0.4997086247086247\n",
      "[[5145    3]\n",
      " [  88    0]]\n",
      "military\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.97      1.00      0.98      5067\n",
      "        True       0.67      0.06      0.11       169\n",
      "\n",
      "    accuracy                           0.97      5236\n",
      "   macro avg       0.82      0.53      0.55      5236\n",
      "weighted avg       0.96      0.97      0.96      5236\n",
      "\n",
      "roc auc score 0.5290924102237123\n",
      "[[5062    5]\n",
      " [ 159   10]]\n",
      "water\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.95      1.00      0.97      4878\n",
      "        True       0.88      0.29      0.44       358\n",
      "\n",
      "    accuracy                           0.95      5236\n",
      "   macro avg       0.91      0.65      0.71      5236\n",
      "weighted avg       0.95      0.95      0.94      5236\n",
      "\n",
      "roc auc score 0.6451105293175837\n",
      "[[4863   15]\n",
      " [ 253  105]]\n",
      "food\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.95      0.98      0.97      4655\n",
      "        True       0.79      0.62      0.70       581\n",
      "\n",
      "    accuracy                           0.94      5236\n",
      "   macro avg       0.87      0.80      0.83      5236\n",
      "weighted avg       0.94      0.94      0.94      5236\n",
      "\n",
      "roc auc score 0.8005745862073428\n",
      "[[4561   94]\n",
      " [ 220  361]]\n",
      "shelter\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.94      1.00      0.96      4756\n",
      "        True       0.87      0.31      0.46       480\n",
      "\n",
      "    accuracy                           0.93      5236\n",
      "   macro avg       0.90      0.65      0.71      5236\n",
      "weighted avg       0.93      0.93      0.92      5236\n",
      "\n",
      "roc auc score 0.6549787987104009\n",
      "[[4734   22]\n",
      " [ 329  151]]\n",
      "clothing\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.99      1.00      0.99      5162\n",
      "        True       0.69      0.15      0.24        74\n",
      "\n",
      "    accuracy                           0.99      5236\n",
      "   macro avg       0.84      0.57      0.62      5236\n",
      "weighted avg       0.98      0.99      0.98      5236\n",
      "\n",
      "roc auc score 0.5738400159167304\n",
      "[[5157    5]\n",
      " [  63   11]]\n",
      "money\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.98      1.00      0.99      5127\n",
      "        True       0.80      0.07      0.13       109\n",
      "\n",
      "    accuracy                           0.98      5236\n",
      "   macro avg       0.89      0.54      0.56      5236\n",
      "weighted avg       0.98      0.98      0.97      5236\n",
      "\n",
      "roc auc score 0.5365022018706507\n",
      "[[5125    2]\n",
      " [ 101    8]]\n",
      "missing_people\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.99      1.00      0.99      5180\n",
      "        True       1.00      0.05      0.10        56\n",
      "\n",
      "    accuracy                           0.99      5236\n",
      "   macro avg       0.99      0.53      0.55      5236\n",
      "weighted avg       0.99      0.99      0.99      5236\n",
      "\n",
      "roc auc score 0.5267857142857143\n",
      "[[5180    0]\n",
      " [  53    3]]\n",
      "refugees\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.96      1.00      0.98      5038\n",
      "        True       0.85      0.06      0.10       198\n",
      "\n",
      "    accuracy                           0.96      5236\n",
      "   macro avg       0.91      0.53      0.54      5236\n",
      "weighted avg       0.96      0.96      0.95      5236\n",
      "\n",
      "roc auc score 0.5275792863129107\n",
      "[[5036    2]\n",
      " [ 187   11]]\n",
      "death\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.96      1.00      0.98      4998\n",
      "        True       0.73      0.23      0.35       238\n",
      "\n",
      "    accuracy                           0.96      5236\n",
      "   macro avg       0.85      0.61      0.67      5236\n",
      "weighted avg       0.95      0.96      0.95      5236\n",
      "\n",
      "roc auc score 0.6135454181672669\n",
      "[[4978   20]\n",
      " [ 183   55]]\n",
      "other_aid\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.87      0.99      0.93      4539\n",
      "        True       0.55      0.05      0.08       697\n",
      "\n",
      "    accuracy                           0.87      5236\n",
      "   macro avg       0.71      0.52      0.51      5236\n",
      "weighted avg       0.83      0.87      0.82      5236\n",
      "\n",
      "roc auc score 0.5200914566977791\n",
      "[[4513   26]\n",
      " [ 665   32]]\n",
      "infrastructure_related\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.93      1.00      0.96      4881\n",
      "        True       0.00      0.00      0.00       355\n",
      "\n",
      "    accuracy                           0.93      5236\n",
      "   macro avg       0.47      0.50      0.48      5236\n",
      "weighted avg       0.87      0.93      0.90      5236\n",
      "\n",
      "roc auc score 0.4995902479000205\n",
      "[[4877    4]\n",
      " [ 355    0]]\n",
      "transport\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jens/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/jens/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/jens/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.96      1.00      0.98      4994\n",
      "        True       0.72      0.07      0.13       242\n",
      "\n",
      "    accuracy                           0.96      5236\n",
      "   macro avg       0.84      0.54      0.56      5236\n",
      "weighted avg       0.95      0.96      0.94      5236\n",
      "\n",
      "roc auc score 0.5364892416354171\n",
      "[[4987    7]\n",
      " [ 224   18]]\n",
      "buildings\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.95      1.00      0.97      4947\n",
      "        True       0.68      0.07      0.12       289\n",
      "\n",
      "    accuracy                           0.95      5236\n",
      "   macro avg       0.81      0.53      0.55      5236\n",
      "weighted avg       0.93      0.95      0.93      5236\n",
      "\n",
      "roc auc score 0.5319623301109406\n",
      "[[4938    9]\n",
      " [ 270   19]]\n",
      "electricity\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.98      1.00      0.99      5140\n",
      "        True       0.50      0.04      0.08        96\n",
      "\n",
      "    accuracy                           0.98      5236\n",
      "   macro avg       0.74      0.52      0.53      5236\n",
      "weighted avg       0.97      0.98      0.97      5236\n",
      "\n",
      "roc auc score 0.5204442282749676\n",
      "[[5136    4]\n",
      " [  92    4]]\n",
      "tools\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.99      1.00      1.00      5207\n",
      "        True       0.00      0.00      0.00        29\n",
      "\n",
      "    accuracy                           0.99      5236\n",
      "   macro avg       0.50      0.50      0.50      5236\n",
      "weighted avg       0.99      0.99      0.99      5236\n",
      "\n",
      "roc auc score 0.5\n",
      "[[5207    0]\n",
      " [  29    0]]\n",
      "hospitals\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.99      1.00      0.99      5177\n",
      "        True       0.00      0.00      0.00        59\n",
      "\n",
      "    accuracy                           0.99      5236\n",
      "   macro avg       0.49      0.50      0.50      5236\n",
      "weighted avg       0.98      0.99      0.98      5236\n",
      "\n",
      "roc auc score 0.5\n",
      "[[5177    0]\n",
      " [  59    0]]\n",
      "shops\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.99      1.00      1.00      5209\n",
      "        True       0.00      0.00      0.00        27\n",
      "\n",
      "    accuracy                           0.99      5236\n",
      "   macro avg       0.50      0.50      0.50      5236\n",
      "weighted avg       0.99      0.99      0.99      5236\n",
      "\n",
      "roc auc score 0.5\n",
      "[[5209    0]\n",
      " [  27    0]]\n",
      "aid_centers\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.99      1.00      0.99      5173\n",
      "        True       0.00      0.00      0.00        63\n",
      "\n",
      "    accuracy                           0.99      5236\n",
      "   macro avg       0.49      0.50      0.50      5236\n",
      "weighted avg       0.98      0.99      0.98      5236\n",
      "\n",
      "roc auc score 0.5\n",
      "[[5173    0]\n",
      " [  63    0]]\n",
      "other_infrastructure\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.95      1.00      0.98      4999\n",
      "        True       0.00      0.00      0.00       237\n",
      "\n",
      "    accuracy                           0.95      5236\n",
      "   macro avg       0.48      0.50      0.49      5236\n",
      "weighted avg       0.91      0.95      0.93      5236\n",
      "\n",
      "roc auc score 0.4998999799959992\n",
      "[[4998    1]\n",
      " [ 237    0]]\n",
      "weather_related\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.86      0.96      0.90      3804\n",
      "        True       0.84      0.57      0.68      1432\n",
      "\n",
      "    accuracy                           0.85      5236\n",
      "   macro avg       0.85      0.76      0.79      5236\n",
      "weighted avg       0.85      0.85      0.84      5236\n",
      "\n",
      "roc auc score 0.7644525903341969\n",
      "[[3643  161]\n",
      " [ 614  818]]\n",
      "floods\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.95      1.00      0.97      4822\n",
      "        True       0.89      0.34      0.49       414\n",
      "\n",
      "    accuracy                           0.94      5236\n",
      "   macro avg       0.92      0.67      0.73      5236\n",
      "weighted avg       0.94      0.94      0.93      5236\n",
      "\n",
      "roc auc score 0.6684234096141477\n",
      "[[4804   18]\n",
      " [ 273  141]]\n",
      "storm\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.94      0.99      0.96      4733\n",
      "        True       0.80      0.36      0.50       503\n",
      "\n",
      "    accuracy                           0.93      5236\n",
      "   macro avg       0.87      0.68      0.73      5236\n",
      "weighted avg       0.92      0.93      0.92      5236\n",
      "\n",
      "roc auc score 0.677154692802408\n",
      "[[4688   45]\n",
      " [ 320  183]]\n",
      "fire\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.99      1.00      1.00      5185\n",
      "        True       0.00      0.00      0.00        51\n",
      "\n",
      "    accuracy                           0.99      5236\n",
      "   macro avg       0.50      0.50      0.50      5236\n",
      "weighted avg       0.98      0.99      0.99      5236\n",
      "\n",
      "roc auc score 0.5\n",
      "[[5185    0]\n",
      " [  51    0]]\n",
      "earthquake\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.97      0.99      0.98      4784\n",
      "        True       0.87      0.68      0.76       452\n",
      "\n",
      "    accuracy                           0.96      5236\n",
      "   macro avg       0.92      0.84      0.87      5236\n",
      "weighted avg       0.96      0.96      0.96      5236\n",
      "\n",
      "roc auc score 0.8359002722940776\n",
      "[[4738   46]\n",
      " [ 144  308]]\n",
      "cold\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.98      1.00      0.99      5130\n",
      "        True       0.73      0.10      0.18       106\n",
      "\n",
      "    accuracy                           0.98      5236\n",
      "   macro avg       0.86      0.55      0.59      5236\n",
      "weighted avg       0.98      0.98      0.97      5236\n",
      "\n",
      "roc auc score 0.551496928905072\n",
      "[[5126    4]\n",
      " [  95   11]]\n",
      "other_weather\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.95      1.00      0.97      4962\n",
      "        True       0.67      0.07      0.13       274\n",
      "\n",
      "    accuracy                           0.95      5236\n",
      "   macro avg       0.81      0.54      0.55      5236\n",
      "weighted avg       0.94      0.95      0.93      5236\n",
      "\n",
      "roc auc score 0.5354886921626257\n",
      "[[4952   10]\n",
      " [ 254   20]]\n",
      "direct_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.85      0.98      0.91      4203\n",
      "        True       0.77      0.33      0.46      1033\n",
      "\n",
      "    accuracy                           0.85      5236\n",
      "   macro avg       0.81      0.65      0.68      5236\n",
      "weighted avg       0.84      0.85      0.82      5236\n",
      "\n",
      "roc auc score 0.651101907340882\n",
      "[[4102  101]\n",
      " [ 696  337]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jens/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/jens/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/jens/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/jens/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/jens/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/jens/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/jens/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/jens/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/jens/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/jens/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/jens/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/jens/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/jens/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/jens/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/jens/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "roc_score = []\n",
    "for column in range(35):\n",
    "    print(Y.columns[column])\n",
    "    print(classification_report(Y_test_array[column], Y_pred.T[column]))\n",
    "    roc = roc_auc_score(Y_test_array[column], Y_pred.T[column])\n",
    "    print('roc auc score', roc)\n",
    "    roc_score.append(roc)\n",
    "    print(confusion_matrix(Y_test_array[column], Y_pred.T[column]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average roc 0.5839750112475957\n"
     ]
    }
   ],
   "source": [
    "print('average roc', np.mean(roc_score)) #0.584"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Export your model as a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "outfile = open('MLmodel.pkl', 'wb')\n",
    "pickle.dump(grid_fit.best_estimator_, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Use this notebook to complete `train.py`\n",
    "Use the template file attached in the Resources folder to write a script that runs the steps above to create a database and export a model based on a new dataset specified by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
